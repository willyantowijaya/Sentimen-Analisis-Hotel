{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyORlTQPjkq2"
   },
   "source": [
    "# **Scraping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EArZtemqjuKy"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w6IR3WcRj52F"
   },
   "outputs": [],
   "source": [
    "# Since the page number changes by 5 in the next url page\n",
    "page = np.arange(5, 51, 1)  \n",
    "Name = []\n",
    "Title = []\n",
    "Review = []\n",
    "Dates=[]\n",
    "Rating = []\n",
    "Sents=[]\n",
    "\n",
    "for pages in page:\n",
    "    # for loop to collect the Mutiple Pages as defined\n",
    "    #url = 'https://www.tripadvisor.co.id/Hotel_Review-g297698-d302653-Reviews-or' + str(\n",
    "        #pages\n",
    "    #) + '-Melia_Bali-Nusa_Dua_Nusa_Dua_Peninsula_Bali.html'\n",
    "\n",
    "    url = 'https://www.tripadvisor.co.id/Hotel_Review-g297698-d5039960-Reviews-' + str(\n",
    "        pages\n",
    "    ) + 'Sofitel_Bali_Nusa_Dua_Beach_Resort-Nusa_Dua_Nusa_Dua_Peninsula_Bali.html'\n",
    "\n",
    "    # import the Url details to Python\n",
    "    req = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    html = req.content\n",
    "\n",
    "    # Put it in soup\n",
    "    sleep(randint(1, 2))  \n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # for loop to extract Customer Names\n",
    "    for a in soup.find_all('a', class_='ui_header_link bPvDb'):\n",
    "        Name.append(a.get_text(strip=True))\n",
    "        \n",
    "    # for loop to extract Review Title\n",
    "    for a in soup.find_all('a', class_='fCitC'):\n",
    "        Title.append(a.get_text(strip=True))\n",
    "\n",
    "    # for loop to extract Reviews\n",
    "    for a in soup.find_all('q', class_='XllAv H4 _a'):\n",
    "        Review.append(a.get_text())\n",
    "\n",
    "    # for loop to extract Review Rating\n",
    "    for a in soup.find_all('div', class_='emWez F1'):\n",
    "        # a.span['class'] is used to only select the span class text which has the rating ['ui_bubble_rating', 'bubble_50']\n",
    "        Rating.append(a.span['class'])\n",
    "\n",
    "    for a in soup.findAll('div',{'class':'bcaHz'}):\n",
    "        Dates.append(a.span.text.strip()) \n",
    "\n",
    "for i in range(len(Rating)):\n",
    "    l = Rating[i][1]\n",
    "    l = l[-2:]\n",
    "    Rating[i]=int(l)/10\n",
    "    if Rating[i]>3:\n",
    "       Sents.append('Positive')\n",
    "    else: Sents.append('Negative')\n",
    "\n",
    "for i in range(len(Dates)):\n",
    "    d = Dates[i]\n",
    "    Dates[i]= d[-8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k22_Gmc3j8Jp"
   },
   "outputs": [],
   "source": [
    "len(Sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXX8w-g8j8yc"
   },
   "outputs": [],
   "source": [
    "# Data Frame created for teh details extracted \n",
    "df=pd.DataFrame({'Customer_name':Name,'Date': Dates, 'Rating':Rating,'Review_Title':Title,'Review':Review,'Sentimens':Sents})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q7fbPB-YqeUu"
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"hotel_reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xY7hO_h8kKXV"
   },
   "source": [
    "Scrape unames, titles, reviews, dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zDQdZ3LjkIqw"
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from time import sleep\n",
    "\n",
    "unames=[]\n",
    "ratings=[]\n",
    "titles=[]\n",
    "reviews =[]\n",
    "dates=[]\n",
    "sents=[]\n",
    "#cnt=1\n",
    "\n",
    "for link in links :\n",
    "  #print(cnt)\n",
    "  #cnt++\n",
    "  d = [10,30,50,70]\n",
    "  headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.59'}\n",
    "  htmls = requests.get(link.format(i for i in d),headers=headers)\n",
    "  #htmls = requests.get(link.format (i for i in d),headers=headers)\n",
    "  sleep(randint(1,2))\n",
    "  bsobj2 = soup(htmls.content,'lxml')\n",
    "  for uname in bsobj2.findAll('a',{'class':'ui_header_link bPvDb'}):\n",
    "    unames.append(uname.text.strip())\n",
    "  for rating in bsobj2.findAll('div',{'class':'emWez F1'}):\n",
    "    ratings.append(rating.span[\"class\"])\n",
    "  for title in bsobj2.findAll('a',{'class':'fCitC'}):\n",
    "    titles.append(title.text.strip()) \n",
    "  for review in bsobj2.findAll('q',{'class':'XllAv H4 _a'}):\n",
    "    reviews.append(review.text.strip())  \n",
    "  for date in bsobj2.findAll('div',{'class':'bcaHz'}):\n",
    "    dates.append(date.span.text.strip()) \n",
    "#reviews \n",
    "for i in range(len(ratings)):\n",
    "  l = ratings[i][1]\n",
    "  l = l[-2:]\n",
    "  ratings[i]=int(l)/10\n",
    "  if ratings[i]>2:\n",
    "    sents.append('Positive')\n",
    "  else: sents.append('Negative')\n",
    "#for i in range(len(dates)):\n",
    "  d=dates[i]\n",
    "  dates[i]=d[-8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iyNGzN-vA30T",
    "outputId": "a7e706a4-b8b4-46d8-fd01-7a6d27044451"
   },
   "outputs": [],
   "source": [
    "len(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPQ5O78MkZE1"
   },
   "source": [
    "Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d3eKd_20kbxt"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dict={'Username':unames, 'Date':dates, 'Rating':ratings, 'Title':titles, 'Review':reviews, 'Sentimens':sents}\n",
    "#dict={'Username':unames, 'Date':dates, 'Title':titles, 'Review':reviews, 'Sentimens':sents}\n",
    "#dict={'Username':unames, 'Date':dates, 'Title':titles, 'Review':reviews}\n",
    "df = pd.DataFrame(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WzixyyEekcaM"
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"test6.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nip3RGHgur6"
   },
   "source": [
    "# **Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2zrZMPJQm4c"
   },
   "source": [
    "Baca data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "KOS6UreTQmFV",
    "outputId": "b1d57547-fc2b-40d3-cab3-98e597429134"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "test_data = pd.read_csv('hotel_reviews.csv')\n",
    "\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiK-8zXntFmd"
   },
   "source": [
    "Library Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E4ULSuqytIwp",
    "outputId": "1ccc055c-13e9-4a66-9faf-ab4de67a7bf2"
   },
   "outputs": [],
   "source": [
    "!pip install Sastrawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2QpKT5KNghn8",
    "outputId": "3ffa0666-6448-46db-ecd1-856f94551ab0"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5UBbGPSRglGX"
   },
   "outputs": [],
   "source": [
    "import nltk #import library nltk\n",
    "from nltk.tokenize import word_tokenize #import word_tokenize for tokenizing text into words \n",
    "from nltk.tokenize import sent_tokenize #import sent_tokenize for tokenizing paragraph into sentences\n",
    "from nltk.stem.porter import PorterStemmer #import Porter Stemmer Algorithm \n",
    "from nltk.stem import WordNetLemmatizer #import WordNet lemmatizer \n",
    "from nltk.corpus import stopwords #import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory #import Indonesian Stemmer\n",
    "from bs4 import BeautifulSoup\n",
    "import re #import regular expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxr2QfyBg3oz"
   },
   "source": [
    "Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iBfma84Ug5Tr",
    "outputId": "c6210adf-aec6-4d3d-8f64-bbe16d0dda1a"
   },
   "outputs": [],
   "source": [
    "t = test_data['Review'].str.lower()\n",
    "\n",
    "print('Sebelum Case Folding : \\n')\n",
    "print(test_data['Review'].head(5))\n",
    "\n",
    "print('Sesudah Case Folding : \\n')\n",
    "print(t.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGacQW2Cg8uW"
   },
   "source": [
    "Tokenizing, Filtering, Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iVhH6x4F46q-"
   },
   "outputs": [],
   "source": [
    "emote = pd.read_csv('master_emoji.csv', encoding= 'unicode_escape')\n",
    "emote_drop = ['ID', 'Sentiment', 'Makna Emoji', 'Special Tag']\n",
    "emote = emote.drop(columns=emote_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3rUjWaWT7dY5",
    "outputId": "b7ef258c-f1ed-4458-eac6-033da5a13449"
   },
   "outputs": [],
   "source": [
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = (stopwords.words('indonesian'))\n",
    "tok = WordPunctTokenizer()\n",
    "\n",
    "\n",
    "def proses_teks(teks):\n",
    "    soup = BeautifulSoup(teks, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    try:\n",
    "        teks = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        teks = souped\n",
    "    teks_bersih = ' '.join([word for word in teks.split() if word not in stop_words])\n",
    "    teks_bersih = ' '.join([word for word in teks_bersih.split() if word not in emote])\n",
    "    teks_bersih = stemmer.stem(teks_bersih)\n",
    "    return (\" \".join([x for x in tok.tokenize(teks_bersih) if len(x) > 1])).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W__71uCr7s5c",
    "outputId": "49834398-aad2-4705-c03c-d91a5dd75af6"
   },
   "outputs": [],
   "source": [
    "data=[]\n",
    "for x in test_data.Review:\n",
    "  data.append(proses_teks(x))\n",
    "  print(len(data),'/',len(test_data))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uf4g-_hKCfCR",
    "outputId": "90fa05e9-22ec-4635-80de-21071ed6e0a9"
   },
   "outputs": [],
   "source": [
    "test_data.Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2GlUtchTZCk"
   },
   "source": [
    "Normalisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "id": "1NcPNV45A0cm",
    "outputId": "824b7212-a1df-4c16-ccf2-4441151ed791"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = test_data\n",
    "slang = pd.read_excel('kamus.xlsx')\n",
    "df['normal'] = data\n",
    "for idx, row in slang.iterrows():\n",
    "    df['normal'] = df.normal.str.replace(r\"\\b\"+row['before']+r\"\\b\", row['after'], regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MA7rq0QoTOIE"
   },
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['normal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "id": "CGywvkSziK-F",
    "outputId": "5b2ddcb6-4b45-4c47-9b2c-2f9efd728343"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxclfNvjijAa"
   },
   "source": [
    "Save Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1MPbCbb1ild_"
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"text_preprocessing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpYcuSMfew27"
   },
   "source": [
    "# **SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPasLdx5ewX7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0eb72d61fUCd"
   },
   "outputs": [],
   "source": [
    "df = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "id": "ERJ-rSJHii6X",
    "outputId": "7fa4bd04-a3e6-4147-c3f2-54c6aaae5c0b"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKPR2Ozpfeoo"
   },
   "source": [
    "SVM Data Prepocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2tYYfvM7fiiG"
   },
   "outputs": [],
   "source": [
    "X = df['Review']\n",
    "y = df['Sentimens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 658
    },
    "id": "jjeI4dEvi5Do",
    "outputId": "aa085934-1dc3-4281-bdff-1df53ea648a3"
   },
   "outputs": [],
   "source": [
    "df[df.Sentimens=='Negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M71bdrcZfm7L"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xK-pMJ4dfrPW"
   },
   "source": [
    "Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L933N_X7kk32",
    "outputId": "b74cd099-6a99-48d7-daa0-321c3b153db3"
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NGSL5PXgfq1x"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf= TfidfVectorizer()\n",
    "svc = SVC(kernel='linear')\n",
    "model= Pipeline([('vectorizer', tf), ('classifier', svc)])\n",
    "model.fit(X_train, y_train)\n",
    "result= model.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-m9b3L0nVxtI",
    "outputId": "a7b39cb2-0079-4df7-b564-a3ec8168aa83"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report=classification_report(y_test, result,output_dict=True)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "id": "sUUYIyubedE5",
    "outputId": "561674cc-2f50-4ca8-eaa8-4e99048f2cb8"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from wordcloud import STOPWORDS\n",
    " \n",
    "# Wordcloud with positive tweets\n",
    "positive_tweets = df['Review'][df[\"Sentimens\"] == 'Positive']\n",
    "stop_words = [\"https\", \"co\", \"RT\"] + list(STOPWORDS)\n",
    "positive_wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\", stopwords = stop_words).generate(str(positive_tweets))\n",
    "plt.figure()\n",
    "plt.title(\"Positive Tweets - Wordcloud\")\n",
    "plt.imshow(positive_wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "id": "jh6F-UOae9FL",
    "outputId": "6b9405b2-ac11-45ca-9734-092c8dd2edee"
   },
   "outputs": [],
   "source": [
    "# Wordcloud with negative tweets\n",
    "negative_tweets = df['Review'][df[\"Sentimens\"] == 'Negative']\n",
    "stop_words = [\"https\", \"co\", \"RT\"] + list(STOPWORDS)\n",
    "negative_wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\", stopwords = stop_words).generate(str(negative_tweets))\n",
    "plt.figure()\n",
    "plt.title(\"Negative Tweets - Wordcloud\")\n",
    "plt.imshow(negative_wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHmLVVjwfwgx"
   },
   "source": [
    "Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FflJitrefyzx"
   },
   "outputs": [],
   "source": [
    "y_pred = svclassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5510fiAff4eE"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "eyORlTQPjkq2",
    "0nip3RGHgur6"
   ],
   "name": "Copy of Skripsi.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
